<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A Survey on Facial Expression Recognition of Static and Dynamic Emotions">
  <meta name="keywords" content="Facial Expression Recognition, Static and Dynamic Emotions, Survey, Affective Computing">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>A Survey on Facial Expression Recognition of Static and Dynamic Emotions</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Facial Expression Recognition of Static and Dynamic Emotions: A Tutorial and Review</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="mailto:yanwang19@fudan.edu.cn">Yan Wang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="mailto:sqyan19@fudan.edu.cn">Shaoqi Yan</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="mailto:yang_liu20@fudan.edu.cn">Yang Liu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="mailto:wsong@shou.edu.cn">Wei Song</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="mailto:jingliu19@fudan.edu.cn">Jing Liu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="mailto:ychang24@m.fudan.edu.cn">Yang Chang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="mailto:xjmai23@m.fudan.edu.cn">Xinji Mai</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="mailto:huxp@bit.edu.cn">Xiping Hu</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="mailto:wqzhang@fudan.edu.cn">Wenqiang Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="mailto:ganzhongxue@fudan.edu.cn">Zhongxue Gan</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Fudan University, </span>
            <span class="author-block"><sup>2</sup>Shanghai Ocean University, </span>
            <span class="author-block"><sup>3</sup>Beijing Institute of Technology</span>
          </div>

          <!--<div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="./static/files/Facial_Expression_Recognition_of_Static_and_Dynamic_Emotions__A_Tutorial_and_Review.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
            </div>
          -->
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="Abstract" style="margin-top: -20px;">
  <div class="container is-max-desktop">

    <!-- Taxonomy Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Taxonomy Overview</h2>
        <div class="content has-text-justified">
          <img src="./static/images/taxonomy_00.png"
          class="interpolation-image"
          alt="Taxonomy Overview Image."/>
          <p> 
            Taxonomy of FER of static and dynamic emotions. We present a hierarchical taxonomy that categorizes existing
            FER models by input type, task challenges, and network structures within a systematic framework, aiming to provide a
            comprehensive overview of the current FER research landscape. First, we have introduced datasets, metrics, and workflow
            (including literature and codes) into a public GitHub repository3 (Sec. 1, 2, 3). Then, image-based SFER (Sec. 4) and video-
            based DFER (Sec. 5) overcome different task challenges using various learning strategies and model designs. Following,
            we analyzed recent advances of FER on benchmark datasets (Sec. 6). Finally, we discussed and concluded some important
            issues and potential trends in FER, highlighting directions for future developments (Sec. 7, 8, 9).
          </p>
        </div>
      </div>
    </div>
    <!--/ Taxonomy Overview. -->

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Facial expression recognition (FER) is pivotal in analyzing emotional states from static images and dynamic sequences,
leveraging AI technologies to enhance anthropomorphic communication among humans, robots, and digital avatars. As the field
evolves from controlled laboratory environments to more complex in-the-wild scenarios, existing FER-related reviews fail to adequately
address the task challenges encountered in new contexts. This paper offers a comprehensive survey of both image-based static FER
(SFER) and video-based dynamic FER (DFER) methods, analyzing from model-oriented development to challenge-focused
categorization. 
        </p>
          <p> 
            We begin with a critical comparison of recent reviews, an introduction to common datasets and evaluation criteria, and
            an in-depth workflow on FER to establish a robust research foundation. We then systematically review representative approaches
            addressing eight main challenges in SFER (such as expression disturbance, uncertainties, compound emotions, and cross-domain
            inconsistency) as well as seven main challenges in DFER (such as key frame sampling, expression intensity variations, and
            cross-modal alignment). Additionally, we analyze recent advancements, benchmark performances, major applications, and ethical
            considerations. Finally, we propose five promising future directions and development trends to guide ongoing research. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section" style="margin-top: -20px;">
  <div class="container is-max-desktop">

    <!-- Datasets. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Comparisons with SOTA FER-related reviews</h2>
        <img src="./static/images/comparisons.png"
        class="interpolation-image"
        alt="Comparisons with SOTA FER-related reviews."/>
      </div>
    </div>
    <!--/ Datasets. -->

    <!-- Datasets. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Datasets</h2>
        <img src="./static/images/DatasetExample_00.jpg"
        class="interpolation-image"
        alt="Datasets."/>
        <div class="content has-text-justified">
          <p>
            Image-based static facial frames (Above) and video-based dynamic facial sequences (Below) of seven basic emotions
            in the lab and wild. Samples are from (a) JAFFE [27], (b) CK+ [28], (c) SFEW [29], (d) ExpW [30], (e) RAF-DB [31], (f)
            AffectNet [32], (g) EmotioNet [33], (h) CK+ [28], (i) Oulu-CASIA [34], (j) DFEW [15], (k) FERV39k [35], and (l) MAFW [36].
          </p>
        </div>
      </div>
    </div>
    <!--/ Datasets. -->

     <!-- Workflow. -->
     <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Workflow of Generic Facial Expression Recognition</h2>
        <img src="./static/images/Tutorial_00.jpg"
        class="interpolation-image"
        alt="Datasets."/>
        <div class="content has-text-justified">
          <p>
            The workflow and main components of generic facial expression recognition.
          </p>
        </div>
      </div>
    </div>
    <!--/ Workflow. -->

    <div class="columns is-centered">
      <!-- Image-based static facial expression recognition. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Image-based Static FER</h2>
          <p>
            Image-based static facial expression recognition (SFER) involves extracting features from a single image, which captures complex spatial information that related to facial expressions, such as landmarks, and their geometric structures and relationships.
In the following, we will first introduce the general architecture of SFER, and then elaborate specific design of SFER methods from the challenge-solving perspectives, including disturbance-invariant SFER, 3D SFER, uncertainty-aware SFER, compound SFER, cross-domain SFER, limited-supervised SFER, and cross-modal SFER.
          </p>
          <h2 class="title is-4">General SFER</h2>
          <img src="./static/images/gernearl_based_SFER_00.jpg"
          class="interpolation-image"
          alt="General SFER Image."/>
          <p>The architecture of general SFER. Figure is reproduced based on (a) CNN-based model, (b) GCN-based model, and (c) Transformer-based model</p>
          <h2 class="title is-4">Disturbance-invariant SFER</h2>
          <img src="./static/images/Disturbance_invariant_SFER_00.jpg"
          class="interpolation-image"
          alt="Disturbance-invariant SFER Image."/>
          <p>The architecture of disturbance-invariant SFER. Figure is reproduced based on (a) Attention-based model (AMP-Net) and (b) Decomposition-based model.</p>
        <h2 class="title is-4">3D SFER</h2>
        <img src="./static/images/3D_SFER_00.jpg"
        class="interpolation-image"
        alt="3D SFER Image."/>
        <p>The architecture of 3D SFER. Figure is reproduced based on (a) GAN-based learning (GAN-Int) and (b) Multi-view learning (MV-CNN).</p>
      <h2 class="title is-4">Uncertainty-aware SFER</h2>
      <img src="./static/images/Uncertainy_aware_SFER_00.jpg"
      class="interpolation-image"
      alt="Uncertainty-aware SFER Image."/>
      <p>The architecture of uncertainty-aware SFER. Figure is reproduced based on (a) the label uncertainty learning (LA-Net) and (b) data uncertainty learning(LNSU-Net).</p>
      <h2 class="title is-4">Compound SFER</h2>
      <p>Compound emotions refer to complex emotional states formed by the combination of at least two basic emotions, which are not independent, discrete categories but exist within a continuous emotional spectrum composed of multiple dimensions. Compared with discrete "basic" emotions or a few dimensions, compound emotions provide a more accurate representation of the diversity and continuity of human complex emotions.</p>
      <h2 class="title is-4">Cross-domain SFER</h2>
      <img src="./static/images/crossdomain_sfer.png"
      class="interpolation-image"
      alt="Cross-domain SFER Image."/>
      <p>The architecture of cross-domain SFER. Figure is reproduced based on (a) the transfer learning-based model (CSRL) and (b) the adaption learning-based model (AGRA).</p>
      <h2 class="title is-4">Weak-supervised SFER</h2>
      <img src="./static/images/fig7-Semi-supervised_SFER_00.jpg"
      class="interpolation-image"
      alt="Weak-supervised SFER Image."/>
      <p>The architecture of weak-supervised SFER. Figure is reproduced based on the Ada-CM.</p>
      <h2 class="title is-4">Cross-modal SFER</h2>
      <img src="./static/images/CEprompt.png"
      class="interpolation-image"
      alt="Cross-modal SFER Image."/>
      <p>The architecture of cross-modal SFER. Figure is reproduced based on the CEprompt.</p>
    </div>
      </div>
    <!--/ Image-based static facial expression recognition. -->

    <!-- Video-based dynamic facial expression recognition. -->
      <div class="column">
        <h2 class="title is-3">Video-based Dynamic Facial Expression Recognition</h2>
        <div class="columns is-centered">
         <div class="column content">
            <p>
              The video-based DFER involves analyzing facial expressions that change over time, necessitating a framework that effectively integrates spatial and temporal information. The core objective of DFER is to extract and learn the features of expression changes from video sequences or image sequences. Due to the complexity and diversity of input video or image sequences, DFER faces various task challenges. Based on different solution approaches, these challenges can be categorized into seven basic types: general DFER, sampling-based DFER, expression intensity-aware DFER, multi-modal DFER, static to dynamic FER, self-supervised DFER, and cross-modal DFER.
            </p>
            <h2 class="title is-4">General DFER</h2>
            <img src="./static/images/generaldfer.png"
            class="interpolation-image"
            alt="General DFER Image."/>
            <p>The architecture of general DFER. Figure is reproduced based on (a) CNN-RNN based model (SAANet) and (b) the transformer-based model (EST).</p>
            <h2 class="title is-4">Sampling-based DFER</h2>
            <img src="./static/images/fig9-Sampling-based_dfer_00.jpg"
            class="interpolation-image"
            alt="Sampling-based DFER Image."/>
            <p>The architecture of sampling-based DFER. Figure is reproduced based on explainable sampling (Freq-HD).</p>
            <h2 class="title is-4">Expression Intensity-aware DFER</h2>
            <p>Facial expressions are inherently dynamic, with intensity either gradually shifting from neutral to peak and back or abruptly transitioning from peak to neutral, making the accurate capture of these fluctuations essential for understanding expression dynamics.</p>
            <h2 class="title is-4">Static to Dynamic FER</h2>
            <p>The static to dynamic FER utilized the high-performance SFER knowledge to explore appearance features and dynamic dependencies. </p>
            <h2 class="title is-4">Multi-modal DFER</h2>
            <img src="./static/images/multi_modal_fusion_dfer.png"
            class="interpolation-image"
            alt="Multi-modal DFER Image."/>
            <p>The architecture of multi-modal DFER. Figure is reproduced based on the fusion-based model (T-MEP).</p>
            <h2 class="title is-4">Self-supervised DFER</h2>
            <img src="./static/images/Self_supervised_DFER_00.jpg"
            class="interpolation-image"
            alt="Self-supervised DFER Image."/>
            <p>The architecture of self-supervised DFER. This is reproduced based on the MAE-DFER.</p>
            <h2 class="title is-4">Visual-Language DFER</h2>
            <img src="./static/images/Figure_ DFER_CLIP.png"
            class="interpolation-image"
            alt="Visual-Language DFER Image."/>
            <p>The architecture of vision-language DFER. Figure is reproduced based on DFER-CLIP.</p>
          </div>

        </div>
      </div>
  <!--/ Video-based dynamic facial expression recognition. -->
    </div>

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{,
  author    = {Yan Wang, Shaoqi Yan, Yang Liu, Wei Song, Jing Liu, Yang Chang, Xinji Mai, Xiping Hu, Wenqiang Zhang, Zhongxue Gan},
  title     = {A Survey on Facial Expression Recognition of Static and Dynamic Emotions},
  journal   = {},
  year      = {},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/files/Facial_Expression_Recognition_of_Static_and_Dynamic_Emotions__A_Tutorial_and_Review.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is website adapted from
             <a href="https://nerfies.github.io/" target="_blank" rel="noopener">Nerfies</a>, 
            licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
            Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
